<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/tags">Tags</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-page6" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/06/15/page6/" class="article-date">
  <time class="dt-published" datetime="2022-06-15T05:48:55.000Z" itemprop="datePublished">2022-06-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/06/15/page6/">Kafka API使用方法</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>生产者API<br>1.新建Maven项目，配置pom.xml<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)29.png"></p>
<p><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)30.png"><br>2.新建ProducerDemo类，ProducerCallbackDemo类<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)31.png"><br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)32.png"><br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)33.png"><br>3.生产者原理<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)34.png"><br>（1）Kafka生产者有两个线程，一个为主线程，一个为Sender线程（它是一个守护线程）<br>（2）消息发送后首先经历拦截器，在拦截器中可以对消息做一些统一的操作，比如：加上统一的标识，然后会被序列化为字节流<br>（3）到达分区器，分区器主要是对消息去哪个分区做规划<br>（4）做好分区规划后，消息到达了消息累加器，累加器是一个双端队列（每一个队列对应一个分区），然后将消息封装到ProducerBatch,ProducerBatch也是一个多消息的容器，它是可以不断想里面加入消息的。让消息变成一批次，一批次的。<br>（5）Sender线程主动从消息累加器中取消息，然后将上一步Deque&lt;partitionId,ProducerBatch&gt;转换为&lt;Node,List&gt;的形式，然后再将其转换为&lt;Node,Request&gt;,这一步是将Kafka消息从逻辑上转换为物理上的主要转折点。从这步开始，就不会关心分区这个逻辑概念，只会注意要发向哪一台机器了<br>（6）在发送之前，还会将消息转换为Map&lt;Node,Dequen【Requst】&gt;<br>（7）保存在InFlightRequst中，表示消息发送等待回应<br>（8）消息发最终由Selector发送<br>4.重要的生产者参数<br>１.acks<br>这个参数主要用来指定分区中必须要有多少个副本都收到这个消息，生产者客户端才认为这条消息成功写入。它涉及消息的可靠性和吞吐量之间的权衡。acks参数有3种类型的值（都是字符串类型）<br>２.max.request.size<br>这个参数用来限制生产者客户端所能发送消息的最大值<br>３.retry.backoff.ms和<br>retries 参数用来配置生产者重试的次数，默认值为0，也就是说在发生异常的时候不进行重试。<br>４.compression.type<br>这个参数用来指定消息的压缩放方式，默认值是“none”,即默认情况下，消息不会被压缩。<br>５.connections. max.ides.ms<br>这个参数用来指定在多久之后关闭闲置连接<br>６.linger.ms<br>这个参数用来指定生产者发送 ProducerBatch 之前等待更多的消息（ProducerRecord）加入 ProducerBatch 的时间<br>７.receive.buffer.bytes<br>这个参数用来设置 Socket 接收消息缓冲区（SO_RECBUF）的大小<br>８.send.buffer.bytes<br>这个参数用来设置 Socket 发送消息缓冲区（SO_SNDBUF）的大小<br>９.request.timeout.ms<br>这个参数用来配置 Producer 等待请求响应的最长时间</p>
<p>消费者API<br>1.一个正常的消费逻辑需要具备以下几个步骤:<br>(1)配置消费者客户端参数<br>(2)创建相应的消费者实例;<br>(3)订阅主题;<br>(4)拉取消息并消费;<br>(5)提交消费位移 offset;<br>(6)关闭消费者实例。<br>2.subscribe 有如下重载方法:<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)34.png"><br>3.正则方式订阅主题<br>如果消费者采用的是正则表达式的方式(subscribe(Pattern))订阅, 在之后的过程中,如果有人又创建了新的主题,并且主题名字与正表达式相匹配,那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题,并且可以处理不同的类型,那么这种订阅方式就很有效。<br>4.assign 订阅主题<br>这个方法只接受参数 partitions,用来指定需要订阅的分区集合<br>5.subscribe 与 assign 的区别<br>(1)通过 subscribe()方法订阅主题具有消费者自动再均衡功能 ;<br>在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。<br>当消费组的消费者增加或减少时,分区分配关系会自动调整,以实现消费负载均衡及故障自动转移。<br>(2)assign() 方法订阅分区时,是不具备消费者自动均衡的功能的;<br>其实这一点从 assign()方法参数可以看出端倪,两种类型 subscribe()都有 ConsumerRebalanceListener 类型参数的方法,而 assign()方法却没有。<br>6.消息的消费模式<br>Kafka 中的消费是基于拉取模式的。消息的消费一般有两种模式:推送模式和拉取模式。<br>推模式是服务端主动将消息推送给消费者,而拉模式是消费者主动向服务端发起请求来拉取消息<br>Kafka 中的消息消费是一个不断轮询的过程,消费者所要做的就是重复地调用 poll() 方法, poll() 方法返回的是所订阅的主题(分区)上的一组消息。<br>7.指定位移消费<br>seek() 方法:从特定的位移处开始拉取消息<br>8.再均衡监听器<br>一个消费组中,一旦有消费者的增减发生,会触发消费者组的 rebalance 再均衡;<br>如果 A 消费者消费掉的一批消息还没来得及提交 offset, 而它所负责的分区在 rebalance 中转移给了 B 消费者,则有可能发生数据的重复消费处理。此情形下,可以通过再均衡监听器做一定程度的补救;<br>9.自动位移提交<br>Kafka 消费的编程逻辑中位移提交是一大难点,自动提交消费位移的方式非常简便,它免去了复杂的位移提交逻辑,让编码更简洁。但随之而来的是重复消费和消息丢失的问题。<br>(1)重复消费<br>假设刚刚提交完一次消费位移,然后拉取一批消息进行消费,在下一次自动提交消费位移之前,消费者崩溃了,那么又得从上一次位移提交的地方重新开始消费,这样便发生了重复消费的现象(对于再均衡的情况同样适用)。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小,但这样并不能避免重复消费的发送,而且也会使位移提交更加频繁。<br>(2)丢失消息<br>拉取线程不断地拉取消息并存入本地缓存, 比如在 BlockingQueue 中, 另一个处理线程从缓存中读取消息并进行相应的逻辑处理<br>10.新建ConsumerDemo，ConsumerDemo1，ConsumerTask，ConsumerDemo2，ConsumerSeekOffset类<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)35.png"><br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)36.png"><br>Topic管理API<br>1.KafkaAdminClient 不仅可以用来管理 broker、配置和 ACL (Access Control List),还可用来管理主题)它提供了以下方法:<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)37.png"><br> (1）连接到kafka服务器：<br> (2)创建管理对象，使用get方法获取主题名称<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)38.png"><br>（3）查看topic具体信息<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)39.png"><br>（4）创建topic<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)40.png"><br>（5）运行代码：<br>（1）topic列表获取成功<br>tpc_1，tpc_2信息获取成功<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)41.png"><br>（2）topic创建成功<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)42.png"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/06/15/page6/" data-id="cl4f6hy4q00021ouj5db762ce" data-title="Kafka API使用方法" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-page5" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/06/15/page5/" class="article-date">
  <time class="dt-published" datetime="2022-06-15T05:48:00.000Z" itemprop="datePublished">2022-06-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/06/15/page5/">Kafka命令行操作</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>cd &#x2F;export&#x2F;server&#x2F;kafka&#x2F;bin目录下的命令行工具</p>
<p>kafka-configs.sh	                   用于配置管理<br>kafka-console-consumer.sh	           用于消费消息<br>kafka-console-producer.sh	           用于生产消息<br>kafka-consumer-perf-test.sh	           用于测试消费性能<br>kafka-topics.sh            	           用于管理主题<br>kafka-dump-log.sh	                   用于查看日志内容<br>kafka-server-stop.sh	               用于关闭kafka服务<br>kafka-preferred-replica-election.sh	   用于优先副本的选举<br>kafka-server-start.sh	               用于启动kafka服务<br>kafka-producer-perf-test.sh	           用于测试生产性能<br>kafka-reassign-partitions.sh	       用于分区重分配<br>查看当前可用topic<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)22.png"><br>创建topic，检查是否创建成功<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)23.png"><br>进入cd &#x2F;export&#x2F;data&#x2F;kafka-logs路径下查看分区的分布<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)24.png"></p>
<p><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)25.png"><br>删除topic<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)26.png"><br>查看topic、修改参数<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)27.png"><br>生产者写入数据、消费者拉取数据<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)28.png"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/06/15/page5/" data-id="cl4f6hy4l00011oujfk6h1rix" data-title="Kafka命令行操作" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-page4" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/06/15/page4/" class="article-date">
  <time class="dt-published" datetime="2022-06-15T05:47:42.000Z" itemprop="datePublished">2022-06-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/06/15/page4/">Kafka环境配置</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>1、Kafka环境配置<br>上传文件包 到&#x2F;export&#x2F;server&#x2F;<br>解压文件<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)1.png"><br>创建软连接<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)2.png"><br>进入 &#x2F;export&#x2F;server&#x2F;kafka&#x2F;confifig 修改 配置文件 server.properties<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)3.png"></p>
<p>21 行内容 broker.id&#x3D;0 为依次增长的:0、1、2、3、4,集群中唯一 id 从0开始，每台不能重复<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)4.png"><br>31 行内容 #listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;:9092 取消注释，内容改为：<br>listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;node1:9092<br>PLAINTEXT为通信使用明文（加密ssl）<br>59 行内容 log.dirs&#x3D;&#x2F;tmp&#x2F;kafka-logs 为默认日志文件存储的位置，改为<br>log.dirs&#x3D;&#x2F;export&#x2F;server&#x2F;data&#x2F;kafka-logs<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)5.png"><br>63 行内容为 num.partitions&#x3D;1 是默认分区数<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)6.png"><br>76 行部分 (数据安全性（持久化之前先放到缓存上，从缓存刷到磁盘上interval.messages interval.ms )<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)7.png"></p>
<p>93 行部分（数据保留策略 168&#x2F;24&#x3D;7，1073741824&#x2F;1024&#x3D;1GB，300000ms &#x3D; 300s &#x3D; 5min超过了删掉<br>（最后修改时间还是创建时间–&gt;日志段中最晚的一条消息，维护这个最大的时间戳–&gt;用户无法干预）<br>121 行内容 zookeeper.connect&#x3D;localhost:2181 修改为<br>zookeeper.connect&#x3D;node1:2181,node2:2181，node3:2181<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)8.png"><br>126 行内容 group.initial.rebalance.delay.ms&#x3D;0 修改为<br>group.initial.rebalance.delay.ms&#x3D;3000<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)9.png"><br>给 node2和 node3 scp 分发 kafka<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)10.png"><br>创建软连接<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)11.png"><br>配置 kafka 环境变量（注：可以一台一台配，也可以在 node1 完成后发给 node21 和node2）<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)12.png"><br>重新加载环境变量<br> <img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)13.png"><br>分别在node2 和node3 上修改配置文件<br>路径：<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)14.png"><br>将文件 server.properties 的第 21 行的 broker.id&#x3D;0 修改为 broker.id&#x3D;1 同理 node2 同样操作<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)15.png"><br>将文件 server.properties 的第 31 行的 listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;node1:9092 修改为<br>listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;node2:9092 同理node3 同样操作listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;slave1:9092<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)16.png"><br>启停 kafka (注：kafka 启动需要在 zookeeper 启动的情况下才可)<br>kafka-server-start.sh -daemon &#x2F;export&#x2F;server&#x2F;kafka&#x2F;config&#x2F;server.properties<br>hadoop，zookeeper，kafka启动<br>结果显示：<br>Node1<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)17.png"><br>Node2<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)18.png"><br>Node3<br> <img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)19.png"><br>定制脚本一键启动<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)20.png"><br>放入 &#x2F;bin 路径下<br><img src="/../images/Kafka%E5%88%86%E7%BB%84%E4%BD%9C%E4%B8%9A/(2)21.png"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/06/15/page4/" data-id="cl4f6hy4d00001ouj7bwdalut" data-title="Kafka环境配置" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/05/22/hello-world/" class="article-date">
  <time class="dt-published" datetime="2022-05-22T07:23:00.215Z" itemprop="datePublished">2022-05-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/05/22/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/05/22/hello-world/" data-id="cl3gzd9r20000f4ujfwhlazup" data-title="Hello World" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-page3" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/05/22/page3/" class="article-date">
  <time class="dt-published" datetime="2022-05-22T06:56:32.000Z" itemprop="datePublished">2022-05-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/05/22/page3/">Spark HA &amp; Yarn配置</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>一、Spark(Yarn)<br>在spark-env.sh中配置HADOOP_CONF_DIR和YARN_CONF_DIR<br>spark-env.sh中部分文件<br><img src="/../images/81.png"></p>
<p>连接到YARN中<br><img src="/../images/82.png"><br><img src="/../images/83.png"><br><img src="/../images/84.png"></p>
<p>客户端工具<br><img src="/../images/85.png"><br>以spark-submit 为例<br>bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;node1:7077 xxx.py<br><img src="/../images/86.png"><br><img src="/../images/87.png"><br><img src="/../images/88.png"><br><img src="/../images/89.png"></p>
<p>启动历史服务器<br>cd &#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;sbin<br>.&#x2F;mr-jobhistory-daemon.sh start historyserver </p>
<p>cilent模式测试<br><img src="/../images/90.png"></p>
<p>cluster模式测试<br><img src="/../images/91.png"></p>
<p>访问WebUI界面<br><a target="_blank" rel="noopener" href="http://master:19888/">http://master:19888/</a><br><img src="/../images/92.png"><br><img src="/../images/93.png"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/05/22/page3/" data-id="cl3gzd9rn0003f4uj8ltteicr" data-title="Spark HA &amp; Yarn配置" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-page2" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/05/22/page2/" class="article-date">
  <time class="dt-published" datetime="2022-05-22T06:39:30.000Z" itemprop="datePublished">2022-05-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/05/22/page2/">Spark local &amp; stand-alone配置</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>一、Spark（local）<br>1、安装上传安装包     Anaconda3-2021.05-Linux-x86_64.sh<br>   运行    sh Anaconda3-2021.05-Linux-x86_64.sh<br><img src="/../images/62.png"><br>2、创建虚拟环境 pyspark 基于 python3.8<br>conda create -n pyspark python&#x3D;3.8<br><img src="/../images/63.png"><br>3、切换到虚拟环境内<br>conda activate pyspark<br><img src="/../images/64.png"><br>4、在虚拟环境内安装包<br><img src="/../images/65.png"><br>5、spark 安装<br>将文件上传到 &#x2F;export&#x2F;server 里面 ，解压<br>tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C &#x2F;export&#x2F;server&#x2F;<br><img src="/../images/66.png"><br>6、建立软连接<br>ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark<br><img src="/../images/67.png"><br>7、添加环境变量<br>vim &#x2F;etc&#x2F;profile<br>重新加载环境变量文件<br>source &#x2F;etc&#x2F;profile<br>source ~&#x2F;.bashrc<br><img src="/../images/68.png"><br><img src="/../images/69.png"><br>8、进入 &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F; 文件夹<br>cd &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;<br>开启<br>.&#x2F;pyspark<br><img src="/../images/70.png"><br><img src="/../images/71.png"><br>二、Spark（Stand alone）<br>1、使用scp拷贝Anacanda文件、.condarc、.bashrc和环境变量到node2,node3上<br>（1）、创建虚拟环境pyspark，基于Python 3.8 ：conda create -n pyspark python&#x3D;3.8<br>（2）、切换到虚拟环境内<br><img src="/../images/72.png"><br>（3）、在虚拟环境内安装包：pip install pyhive pyspark jieba -i <a target="_blank" rel="noopener" href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a><br>2、将workers.template改名为workers，并配置内容<br><img src="/../images/73.png"><br>3、将spark-env.sh.template改名为spark-env.sh，并配置内容<br><img src="/../images/74.png"><br>4、Hadoop集群打开，将spark日志路径、权限上传到HDFS上：hadoop fs -mkdir &#x2F;sparklog<br>hadoop fs -chmod 777 &#x2F;sparklog<br>5、将spark-defaults.conf.sh.template改名为spark-defaults.conf，并配置内容<br><img src="/../images/75.png"><br>6、将log4j.properties.template改名为log4j.properties，并配置内容<br><img src="/../images/76.png"><br>7、使用scp拷贝spark到node2,node3上<br>8、在node2、node3上给spark安装目录增加软连接<br>9、启动历史服务器&#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;start-history-server.sh<br><img src="/../images/77.png"><br>10、启动spark的master和worker进程：sbin&#x2F;start-all.sh<br><img src="/../images/78.png"><br>11、查看spark web UI页面（8080）、历史服务器web UI页面（18080）<br><img src="/../images/79.png"><br><img src="/../images/80.png"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/05/22/page2/" data-id="cl3gzd9rb0001f4uj3tz1h5pf" data-title="Spark local &amp; stand-alone配置" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-page" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/05/22/page/" class="article-date">
  <time class="dt-published" datetime="2022-05-22T04:39:54.000Z" itemprop="datePublished">2022-05-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/05/22/page/">Spark基础环境配置</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>一、基础环境<br>1、导入三台虚拟机，三台虚拟机信息汇总<br>主机名	        node1.itcast.cn 	node2.itcast.cn 	node3.itcast.cn<br>IP	            192.168.88.151	    192.168.88.152	    192.168.88.153<br>用户名、密码	root&#x2F;123456	        root&#x2F;123456	        root&#x2F;123456<br>2、集群角色规划<br>服务器	运行角色<br>node1.itcast.cn	namenode（主角色）  datanode（从角色） resourcemanager（主角色） nodemanager（从角色）<br>node2.itcast.cn	secondarynamenode（主角色辅助角色） datanode（从角色） nodemanager（从角色）<br>node3.itcast.cn	datanode （从角色）nodemanager（从角色）<br>3、hosts映射<br><img src="/../images/2.png"><br>4、快捷命令<br>（1）、查看主机名：cat &#x2F;etc&#x2F;hostname<br><img src="/../images/1.png"><br>（2）、查看防火墙状态：systemctl status firewalld.service<br><img src="/../images/3.png"><br>（3）、登录node1：ssh node1    登录node2：ssh node2   登录node3：ssh node3<br><img src="/../images/4.png"><br>（4）、同步时间：ntpdate ntp5.aliyun.com<br><img src="/../images/5.png"><br>二、jdk<br>1、编译环境软件安装目录<br><img src="/../images/6.png"><br>2、上传 jdk-8u241-linux-x64.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下 并解压文件tar -zxvf jdk-8u241-linux-x64.tar.gz<br><img src="/../images/7.png"><br><img src="/../images/8.png"><br>3、配置环境变量   vim &#x2F;etc&#x2F;profile<br><img src="/../images/9.png"><br>4、重新加载环境变量文件    source &#x2F;etc&#x2F;profile<br>5、查看 java 版本号    查看 java 版本号<br><img src="/../images/10.png"><br>6、将 java 传输到 node2 和 node3<br>scp -r &#x2F;export&#x2F;server&#x2F;jdk1.8.0_241&#x2F; root@node2:&#x2F;export&#x2F;server<br>scp -r &#x2F;export&#x2F;server&#x2F;jdk1.8.0_241&#x2F; root@node3:&#x2F;export&#x2F;server<br><img src="/../images/11.png"><br>7、配置node2、node3的环境变量（和上方相同）<br>8、创建软连接（node1、node2、node3）<br><img src="/../images/12.png"><br>9、重新加载环境变量    source &#x2F;etc&#x2F;profile<br><img src="/../images/13.png"><br>三、Hadoop<br>●将Hadoop-3.3.0上传至&#x2F;export&#x2F;server目录下并解压源码包<br> <img src="/../images/14.png"><br>  <img src="/../images/15.png"><br>●修改配置文件进入路径&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;etc&#x2F;hadoop<br>进入vim hadoop-env.sh<br>并在文件末添加<br><img src="/../images/16.png"><br>●Core-site.xml<br>○ 设置默认的文件系统<br><img src="/../images/17.png"><br>○设置hadoop本地保存数据路径<br><img src="/../images/18.png"><br>○设置HDFS web UI用户身份<br><img src="/../images/19.png"><br>○整合hive 用户代理设置<br><img src="/../images/20.png"><br>○文件系统垃圾桶保存时间<br><img src="/../images/21.png"><br>●hdfs-site.xml<br>○设置SNN进程运行机器位置信息<br><img src="/../images/22.png"><br>●mapred-site.xml<br>○设置MR程序默认运行模式： yarn集群模式 local本地模式<br><img src="/../images/23.png"><br>○ MR程序历史服务地址<br><img src="/../images/24.png"><br>○MR程序历史服务器web端地址<br><img src="/../images/25.png"><br>●yarn-site.xml<br>○设置YARN集群主角色运行机器位置<br><img src="/../images/26.png"><br>○是否将对容器实施物理内存限制<br><img src="/../images/27.png"><br>○开启日志聚集<br><img src="/../images/28.png"><br>○设置yarn历史服务器地址<br><img src="/../images/29.png"><br>○历史日志保存的时间 7天<br><img src="/../images/30.png"><br>●workers<br><img src="/../images/31.png"><br>●分发同步hadoop安装包（进入路径cd &#x2F;export&#x2F;server）<br><img src="/../images/32.png"><br>●将hadoop添加到环境变量<br>分别进入node1 node2 node3的profile文件（vim &#x2F;etc&#x2F;profile）<br><img src="/../images/33.png"><br>  ●Hadoop集群启动<br>○首次启动应当格式化namenode<br><img src="/../images/35.png"><br>○使用脚本一键启动，jps查看进程<br><img src="/../images/36.png"><br><img src="/../images/37.png"><br><img src="/../images/38.png"><br><img src="/../images/39.png"><br>●查看hdfs、yarn集群的web UI页面<br><img src="/../images/40.png"><br><img src="/../images/41.png"><br>四、Zookeeper<br>1、基本规划<br>服务器IP	主机名	myid的值<br>192.168.88.151	node1	1<br>192.168.88.152	node2	2<br>192.168.88.153	node3	3<br>2、解压<br>●在node1主机上，上传并解压zookeeper的压缩包到&#x2F;export&#x2F;server路径下去，然后准备进行安装<br>〇进入目录cd &#x2F;export&#x2F;software<br>进行解压<br><img src="/../images/42.png"><br><img src="/../images/43.png"><br>〇建立软链接	<br><img src="/../images/44.png"><br>3、修改配置文件<br>（1）、在&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F;目录下复制zoo_sample.cfg文件为zoo.cfg：cp zoo_sample.cfg zoo.cfg<br><img src="/../images/45.png"><br>（2）、在&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;路径下创建一个zkdatas文件夹：mkdir -p &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;<br><img src="/../images/46.png"><br>（3）、配置zoo.cfg<br><img src="/../images/47.png"><br><img src="/../images/48.png"><br>（4）、在node1的&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;目录下创建一个myid文件，文件内容为1：echo 1 &gt; &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;myid<br><img src="/../images/49.png"><br>4、使用scp拷贝zookeeper安装包到node2,node3上<br>5、在node2、node3上建立软连接<br>6、修改node2、node3的myid值为2、3<br><img src="/../images/50.png"><br><img src="/../images/51.png"><br>7、配置环境变量，重新加载环境变量<br><img src="/../images/52.png"><br>8、三台机器启动zookeeper服务<br>&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh start<br><img src="/../images/53.png"><br><img src="/../images/54.png"><br><img src="/../images/55.png"><br>9、三台主机分别查看启动状态<br>&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh  status<br><img src="/../images/56.png"><br><img src="/../images/57.png"><br><img src="/../images/58.png"><br>10、jps查看进程<br><img src="/../images/59.png"><br><img src="/../images/60.png"><br><img src="/../images/61.png"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/05/22/page/" data-id="cl3gzd9rj0002f4uj32dqfufl" data-title="Spark基础环境配置" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  

</section>
        <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/06/15/page6/">Kafka API使用方法</a>
          </li>
        
          <li>
            <a href="/2022/06/15/page5/">Kafka命令行操作</a>
          </li>
        
          <li>
            <a href="/2022/06/15/page4/">Kafka环境配置</a>
          </li>
        
          <li>
            <a href="/2022/05/22/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2022/05/22/page3/">Spark HA &amp; Yarn配置</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/06/">June 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">May 2022</a></li></ul>
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 By Autoload<br>
      Driven - <a href="https://hexo.io/" target="_blank">Hexo</a>|Theme - <a href="https://github.com/autoload/hexo-theme-auto" target="_blank">Auto</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>


<script src="/js/script.js"></script>




  </div>
</body>
</html>